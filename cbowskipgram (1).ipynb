{"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNqd3ywKXivwfCfspqeU7L/","gpuType":"T4","mount_file_id":"1VU19XIT54N4GvXlad9dAVGIN7m_2Zks6","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8267483,"sourceType":"datasetVersion","datasetId":4908065}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nfolder_path = r'/kaggle/input/got-books'\n\ndata=''\nfor filename in os.listdir(folder_path):\n  filepath = os.path.join(folder_path, filename)\n  if os.path.isfile(filepath) and filename.endswith('.txt'):\n    with open(filepath, 'r', encoding='latin') as file_ref:\n      data += file_ref.read()\n\nprint(f'Length: {len(data)}\\n')\nprint(f'Sample: \\n{data[:1000]}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1710788694755,"user":{"displayName":"KEERTAN PARIKH","userId":"09510881889475511598"},"user_tz":-330},"id":"8zA5fmz0fnw9","outputId":"41389116-7c7d-4463-c529-52abdd53dd76","execution":{"iopub.status.busy":"2024-04-30T01:14:31.364374Z","iopub.execute_input":"2024-04-30T01:14:31.364705Z","iopub.status.idle":"2024-04-30T01:14:31.523576Z","shell.execute_reply.started":"2024-04-30T01:14:31.364676Z","shell.execute_reply":"2024-04-30T01:14:31.522626Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Length: 9778333\n\nSample: \nVersion History: \n2.0 - Reedited 4/25/10 by maelstrom385 \n\n\nA FEAST FOR CROWS\nBook Four: A Song of Ice and Fire \nGeorge R.R. Martin \nPROLOGUE \nDragons, said Mollander. He snatched a withered apple off the ground and tossed it \nhand to hand. \nThrow the apple, urged Alleras the Sphinx. He slipped an arrow from his quiver and nocked \nit to his bowstring. \nI should like to see a dragon. Roone was the youngest of them, a chunky boy still two years \nshy of manhood. I should like that very much. \nAnd I should like to sleep with Roseys arms around me, Pate thought. He shifted restlessly on \nthe bench. By the morrow the girl could well be his. I will take her far from Oldtown, across the \nnarrow sea to one of the Free Cities. There were no maesters there, no one to accuse him. \nHe could hear Emmas laughter coming through a shuttered window overhead, mingled with \nthe deeper voice of the man she was entertaining. She was the oldest of the serving wenches at \nthe Quill and Tankard, fort\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\n\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\n\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\ndef lowercase_and_filter(text):\n    text=text.lower()\n    cleaned_text = re.sub(r'<[^>]*>', '', text)\n    cleaned_text = re.sub(r'\\S+@\\S+', '', cleaned_text)\n    cleaned_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', cleaned_text)\n    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n    cleaned_text = re.sub(r'\\n', ' ', cleaned_text)\n    cleaned_text = re.sub(r'\\t', ' ', cleaned_text)\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n    return cleaned_text\n\ndef tokenize(sen):\n    return word_tokenize(sen)\n\ndef stopwords_removal(sen):\n    stop_words = set(stopwords.words('english'))\n    return [word for word in sen if word not in stop_words]\n\ndef lemmatize(sen):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in sen]\n\ndef preprocess_text(text):\n    cleaned_text = lowercase_and_filter(text)\n    tokenized_text = tokenize(cleaned_text)\n    text_without_stopwords = stopwords_removal(tokenized_text)\n    lemmatized_text = lemmatize(text_without_stopwords)\n    return lemmatized_text\n\npreprocessed_data = preprocess_text(data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13436,"status":"ok","timestamp":1710788709895,"user":{"displayName":"KEERTAN PARIKH","userId":"09510881889475511598"},"user_tz":-330},"id":"b9rSjjpAfkG7","outputId":"e74557a4-87d7-4fe7-f53c-d1a088febb53","execution":{"iopub.status.busy":"2024-04-30T01:15:06.186964Z","iopub.execute_input":"2024-04-30T01:15:06.187360Z","iopub.status.idle":"2024-04-30T01:15:31.021166Z","shell.execute_reply.started":"2024-04-30T01:15:06.187329Z","shell.execute_reply":"2024-04-30T01:15:31.020196Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"print(preprocessed_data[:100])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1710788709895,"user":{"displayName":"KEERTAN PARIKH","userId":"09510881889475511598"},"user_tz":-330},"id":"e9Yh7zIelrXg","outputId":"c062159c-3266-495e-dfdd-e7060975af91","execution":{"iopub.status.busy":"2024-04-30T01:15:31.022893Z","iopub.execute_input":"2024-04-30T01:15:31.023357Z","iopub.status.idle":"2024-04-30T01:15:31.029039Z","shell.execute_reply.started":"2024-04-30T01:15:31.023319Z","shell.execute_reply":"2024-04-30T01:15:31.027950Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['version', 'history', '20', 'reedited', '42510', 'maelstrom385', 'feast', 'crow', 'book', 'four', 'song', 'ice', 'fire', 'george', 'rr', 'martin', 'prologue', 'dragon', 'said', 'mollander', 'snatched', 'withered', 'apple', 'ground', 'tossed', 'hand', 'hand', 'throw', 'apple', 'urged', 'alleras', 'sphinx', 'slipped', 'arrow', 'quiver', 'nocked', 'bowstring', 'like', 'see', 'dragon', 'roone', 'youngest', 'chunky', 'boy', 'still', 'two', 'year', 'shy', 'manhood', 'like', 'much', 'like', 'sleep', 'roseys', 'arm', 'around', 'pate', 'thought', 'shifted', 'restlessly', 'bench', 'morrow', 'girl', 'could', 'well', 'take', 'far', 'oldtown', 'across', 'narrow', 'sea', 'one', 'free', 'city', 'maesters', 'one', 'accuse', 'could', 'hear', 'emmas', 'laughter', 'coming', 'shuttered', 'window', 'overhead', 'mingled', 'deeper', 'voice', 'man', 'entertaining', 'oldest', 'serving', 'wench', 'quill', 'tankard', 'forty', 'day', 'still', 'pretty', 'fleshy']\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_train_data(data, window_size):\n    train_data = []\n    for i in range(window_size, len(data) - window_size):\n        context = [data[j] for j in range(i - window_size, i + window_size + 1) if j != i]\n        target = data[i]\n        train_data.append((context, target))\n    return train_data","metadata":{"execution":{"iopub.status.busy":"2024-04-30T01:16:23.920884Z","iopub.execute_input":"2024-04-30T01:16:23.921630Z","iopub.status.idle":"2024-04-30T01:16:23.927697Z","shell.execute_reply.started":"2024-04-30T01:16:23.921596Z","shell.execute_reply":"2024-04-30T01:16:23.926559Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# SKIPGRAM","metadata":{"id":"03LRb3Ph6LMm"}},{"cell_type":"markdown","source":"# CBOW","metadata":{"id":"fnHIlV3ujnzt"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\n\nclass LoadDataset(Dataset):\n    def __init__(self, train_data, word_to_idx):\n        self.train_data = train_data\n        self.word_to_idx = word_to_idx\n\n    def __len__(self):\n        return len(self.train_data)\n\n    def __getitem__(self, idx):\n        context, target = self.train_data[idx]\n        context_idxs = [self.word_to_idx[word] for word in context]\n        target_idx = self.word_to_idx[target]\n        return torch.tensor(context_idxs), torch.tensor(target_idx)\n\nclass CBOW(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(CBOW, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.output = nn.Linear(embedding_dim, vocab_size)\n\n    def forward(self, context_idxs):\n        embedded_context = self.embedding(context_idxs).mean(dim=1)\n        output = self.output(embedded_context)\n        return output\n\ndef main(data, embedding_dim=100, lr=0.01, epochs=10, batch_size=32, window_size=2):\n    vocab = list(set(data))\n    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n    vocab_size = len(vocab)\n\n    train_data = get_train_data(data, window_size)\n\n    model = CBOW(vocab_size, embedding_dim)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    dataset = LoadDataset(train_data, word_to_idx)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    for epoch in range(epochs):\n        total_loss = 0\n        for context_idxs, target_idx in dataloader:\n            optimizer.zero_grad()\n            output = model(context_idxs)\n            loss = loss_fn(output, target_idx)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(dataloader)\n        print(f'Epoch {epoch + 1}, Average Loss: {avg_loss}')\n\n    return model, word_to_idx, vocab\n\nmodel, word_to_idx, idx_to_word = main(preprocessed_data[:100000])\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":579232,"status":"ok","timestamp":1710789602292,"user":{"displayName":"KEERTAN PARIKH","userId":"09510881889475511598"},"user_tz":-330},"id":"MOmK7pdB10BT","outputId":"a4e7de37-a00a-4729-ceb1-b4bebafc8c21","execution":{"iopub.status.busy":"2024-04-30T01:16:31.016266Z","iopub.execute_input":"2024-04-30T01:16:31.016755Z","iopub.status.idle":"2024-04-30T01:22:27.109974Z","shell.execute_reply.started":"2024-04-30T01:16:31.016719Z","shell.execute_reply":"2024-04-30T01:22:27.108849Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1, Average Loss: 8.159978136291503\nEpoch 2, Average Loss: 6.069935362548828\nEpoch 3, Average Loss: 4.646448402481079\nEpoch 4, Average Loss: 3.8009480448532105\nEpoch 5, Average Loss: 3.3166903911209107\nEpoch 6, Average Loss: 3.005242641143799\nEpoch 7, Average Loss: 2.7972702291107177\nEpoch 8, Average Loss: 2.6344286576652527\nEpoch 9, Average Loss: 2.512125694103241\nEpoch 10, Average Loss: 2.4133790214920046\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\ndef cosine_similarity(vector1, vector2):\n    dot_product = np.dot(vector1, vector2)\n    norm_vector1 = np.linalg.norm(vector1)\n    norm_vector2 = np.linalg.norm(vector2)\n    similarity = dot_product / (norm_vector1 * norm_vector2)\n    return similarity\n\ndef get_word_vector(word, model, word_to_idx):\n    word_idx = word_to_idx[word]\n    return model.embedding(torch.tensor(word_idx)).detach().numpy()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121625,"status":"ok","timestamp":1710789873847,"user":{"displayName":"KEERTAN PARIKH","userId":"09510881889475511598"},"user_tz":-330},"id":"6dPfuQ4NrrB4","outputId":"fde5a6dc-0fb4-4a45-e82c-e0b1adbed7a3","execution":{"iopub.status.busy":"2024-04-30T01:23:07.936307Z","iopub.execute_input":"2024-04-30T01:23:07.937051Z","iopub.status.idle":"2024-04-30T01:23:07.944049Z","shell.execute_reply.started":"2024-04-30T01:23:07.937020Z","shell.execute_reply":"2024-04-30T01:23:07.943052Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def search(query, model, word_to_idx, idx_to_word, n):\n    words = query.split()\n    if len(words) != 3:\n        print(\"Invalid query format. Please provide three words separated by '+' and '-'.\")\n        return\n\n    # Extracting vectors for each word\n    word1_vector = get_word_vector(words[0], model, word_to_idx)\n    word2_vector = get_word_vector(words[1], model, word_to_idx)\n    word3_vector = get_word_vector(words[2], model, word_to_idx)\n\n    # Calculating the vector arithmetic: king - male + female\n    query_vector = word1_vector - word2_vector + word3_vector\n\n    # Finding the most similar word to the query vector\n    similarities = []\n    for idx, vec in enumerate(model.embedding.weight.detach().numpy()):\n        sim = cosine_similarity([query_vector], [vec])[0][0]\n        similarities.append((idx_to_word[idx], sim))\n    similarities.sort(key=lambda x: x[1], reverse=True)\n\n    # Displaying the top 'n' similar words\n    print(f'\\n\\nTop {n} similar words for the query \"{query}\"')\n    for w, sim in similarities[:n]:\n        time.sleep(0.5)\n        print(f\"Word: {w}, Similarity: {sim}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T01:25:49.016756Z","iopub.execute_input":"2024-04-30T01:25:49.017168Z","iopub.status.idle":"2024-04-30T01:25:49.027085Z","shell.execute_reply.started":"2024-04-30T01:25:49.017124Z","shell.execute_reply":"2024-04-30T01:25:49.025923Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# A - B + C format\nsearch(\"person human animal\", model, word_to_idx, idx_to_word, 5)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T01:26:37.693583Z","iopub.execute_input":"2024-04-30T01:26:37.694499Z","iopub.status.idle":"2024-04-30T01:26:43.136294Z","shell.execute_reply.started":"2024-04-30T01:26:37.694455Z","shell.execute_reply":"2024-04-30T01:26:43.135200Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\n\nTop 5 similar words for the query \"person human animal\"\nWord: animal, Similarity: 0.6417559385299683\nWord: person, Similarity: 0.44666817784309387\nWord: ala, Similarity: 0.35783103108406067\nWord: buzz, Similarity: 0.3319343328475952\nWord: beesbury, Similarity: 0.33158421516418457\n","output_type":"stream"}]}]}